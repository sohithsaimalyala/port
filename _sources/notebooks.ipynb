{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Normalizing ths dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "data_path = \"Housing.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Inspect the dataset\n",
    "print(\"Dataset Columns:\")\n",
    "print(data.columns)\n",
    "\n",
    "# Step 2: Create SQLite database and tables\n",
    "conn = sqlite3.connect(\"housing.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Table: PropertyDetails\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS PropertyDetails (\n",
    "    Area REAL,\n",
    "    Bedrooms INTEGER,\n",
    "    Bathrooms INTEGER,\n",
    "    Stories INTEGER,\n",
    "    Parking INTEGER,\n",
    "    Price REAL\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Table: Amenities\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS Amenities (\n",
    "    RowID INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    MainRoad BOOLEAN,\n",
    "    GuestRoom BOOLEAN,\n",
    "    Basement BOOLEAN,\n",
    "    HotWaterHeating BOOLEAN,\n",
    "    AirConditioning BOOLEAN,\n",
    "    PrefArea BOOLEAN\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Table: Furnishing\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS Furnishing (\n",
    "    RowID INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    FurnishingStatus TEXT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "print(\"Database schema created successfully.\")\n",
    "\n",
    "# Step 3: Populate the PropertyDetails table\n",
    "property_details = data[['area', 'bedrooms', 'bathrooms', 'stories', 'parking', 'price']]\n",
    "property_details.to_sql('PropertyDetails', conn, if_exists='replace', index=True)  # Save with default rowid\n",
    "\n",
    "# Step 4: Populate the Amenities table\n",
    "amenities = data[['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']]\n",
    "amenities.to_sql('Amenities', conn, if_exists='replace', index=True)\n",
    "\n",
    "# Step 5: Populate the Furnishing table\n",
    "furnishing = data[['furnishingstatus']]\n",
    "furnishing.to_sql('Furnishing', conn, if_exists='replace', index=True)\n",
    "\n",
    "print(\"Data populated successfully into normalized database.\")\n",
    "\n",
    "# Step 6: Verify the database with a joined query\n",
    "query = \"\"\"\n",
    "SELECT pd.rowid AS PropertyID, pd.Area, pd.Bedrooms, pd.Bathrooms, pd.Stories, pd.Parking, pd.Price,\n",
    "       a.MainRoad, a.GuestRoom, a.Basement, a.HotWaterHeating, a.AirConditioning, a.PrefArea,\n",
    "       f.FurnishingStatus\n",
    "FROM PropertyDetails pd\n",
    "JOIN Amenities a ON pd.rowid = a.rowid\n",
    "JOIN Furnishing f ON pd.rowid = f.rowid\n",
    "\"\"\"\n",
    "result = pd.read_sql_query(query, conn)\n",
    "\n",
    "print(\"Joined Data:\")\n",
    "print(result.head())\n",
    "\n",
    "# Step 7: Close the connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Write SQL join statement to fetch data from the database and into Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Connect to the database\n",
    "db_path = \"housing.db\"  # Path to the SQLite database\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Step 2: SQL join query\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    pd.rowid AS PropertyID,\n",
    "    pd.Area,\n",
    "    pd.Bedrooms,\n",
    "    pd.Bathrooms,\n",
    "    pd.Stories,\n",
    "    pd.Parking,\n",
    "    pd.Price,\n",
    "    a.MainRoad,\n",
    "    a.GuestRoom,\n",
    "    a.Basement,\n",
    "    a.HotWaterHeating,\n",
    "    a.AirConditioning,\n",
    "    a.PrefArea,\n",
    "    f.FurnishingStatus\n",
    "FROM PropertyDetails pd\n",
    "JOIN Amenities a ON pd.rowid = a.rowid\n",
    "JOIN Furnishing f ON pd.rowid = f.rowid\n",
    "\"\"\"\n",
    "\n",
    "# Step 3: Fetch data into Pandas DataFrame\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Step 4: Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# Step 5: Display the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Explore the data to determine if you need to stratify it by some attribute when doing train/test split. Perform the train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "data_path = \"Housing.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Step 2: Explore the target variable\n",
    "target = 'price'  # Assuming 'price' is the target variable\n",
    "print(f\"Target Column: {target}\")\n",
    "print(data[target].describe())\n",
    "\n",
    "# Plot the distribution of the target variable\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(data[target], bins=30, color='blue', edgecolor='black', alpha=0.7)\n",
    "plt.title(\"Target Variable Distribution\")\n",
    "plt.xlabel(\"Price\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Check for categorical variables for potential stratification\n",
    "categorical_features = ['furnishingstatus']\n",
    "for feature in categorical_features:\n",
    "    print(f\"\\nDistribution of {feature}:\")\n",
    "    print(data[feature].value_counts())\n",
    "    data[feature].value_counts().plot(kind='bar', title=f\"Distribution of {feature}\")\n",
    "    plt.show()\n",
    "\n",
    "# Step 3: Determine the need for stratification\n",
    "# If 'furnishingstatus' is imbalanced, stratify by it\n",
    "stratify_col = 'furnishingstatus' if data['furnishingstatus'].nunique() > 1 else None\n",
    "\n",
    "# Step 4: Perform train/test split\n",
    "X = data.drop(columns=[target])  # Features\n",
    "y = data[target]                 # Target variable\n",
    "\n",
    "if stratify_col:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=data[stratify_col]\n",
    "    )\n",
    "    print(f\"Data stratified by {stratify_col}\")\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(\"Data split without stratification\")\n",
    "\n",
    "# Step 5: Output summary\n",
    "print(f\"Training data size: {X_train.shape}\")\n",
    "print(f\"Testing data size: {X_test.shape}\")\n",
    "\n",
    "# Save the split datasets\n",
    "X_train.to_csv(\"X_train.csv\", index=False)\n",
    "X_test.to_csv(\"X_test.csv\", index=False)\n",
    "y_train.to_csv(\"y_train.csv\", index=False)\n",
    "y_test.to_csv(\"y_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Explore the data using yprofile and correlation matrix. Make observations about features, distributions, capped values, and missing values. Create a list of data cleanup tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure inline plotting for Jupyter Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the dataset\n",
    "data_path = \"Housing.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Step 1: Convert `furnishingstatus` to numerical values\n",
    "if 'furnishingstatus' in data.columns:\n",
    "    data['furnishingstatus'] = data['furnishingstatus'].map({\n",
    "        'unfurnished': 0,\n",
    "        'semi-furnished': 1,\n",
    "        'furnished': 2\n",
    "    })\n",
    "    print(\"Furnishingstatus column converted to numerical values.\")\n",
    "else:\n",
    "    print(\"Column 'furnishingstatus' not found in the dataset.\")\n",
    "\n",
    "# Convert `yes`/`no` values to `1`/`0`\n",
    "for col in data.select_dtypes(include=['object']).columns:\n",
    "    if data[col].str.lower().isin(['yes', 'no']).any():\n",
    "        data[col] = data[col].map({'yes': 1, 'no': 0})\n",
    "        print(f\"Column '{col}' converted to numerical values.\")\n",
    "\n",
    "# Step 2: Correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlation_matrix = data.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Extract and process top correlations\n",
    "# Get the correlation matrix as a DataFrame\n",
    "correlation_df = correlation_matrix.reset_index().melt(id_vars='index', var_name='Feature2', value_name='Correlation')\n",
    "\n",
    "# Rename columns for clarity\n",
    "correlation_df.rename(columns={'index': 'Feature1'}, inplace=True)\n",
    "\n",
    "# Remove self-correlations\n",
    "correlation_df = correlation_df[correlation_df['Feature1'] != correlation_df['Feature2']]\n",
    "\n",
    "# Sort by absolute correlation values in descending order\n",
    "correlation_df['AbsCorrelation'] = correlation_df['Correlation'].abs()\n",
    "top_correlations = correlation_df.sort_values(by='AbsCorrelation', ascending=False).head(10)\n",
    "\n",
    "# Display top correlations\n",
    "print(\"\\nTop correlations (excluding self-correlations):\")\n",
    "print(top_correlations[['Feature1', 'Feature2', 'Correlation']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Experiment #1: Create a pipeline for preprocessing (StandardScaler, MinMaxScaler, LogTransformation, OneHotEncoding) and Logistic Regression. Log F1-score/(TP,TN,FN,FP)  in MLFlow on DagsHub. – Cross validation 3/10 folds. Results—mean/std of CV results and results on the whole training data – add in parameter hyper tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment1\n",
    "\n",
    "import os\n",
    "import mlflow\n",
    "\n",
    "# Set up MLFlow tracking URI and authentication\n",
    "MLFLOW_TRACKING_URI = \"https://dagshub.com/sohithsaimalyala/Project.mlflow\"\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = 'sohithsaimalyala'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = '29f357e14d4829f0c3e67f7e44b6391e7984e0cd'\n",
    "\n",
    "# Configure MLFlow\n",
    "mlflow.set_tracking_uri(uri=MLFLOW_TRACKING_URI)\n",
    "\n",
    "# Experiment name\n",
    "experiment_name = \"Housing_Prediction\"\n",
    "\n",
    "# Set or create experiment\n",
    "try:\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "except mlflow.exceptions.MlflowException:\n",
    "    print(f\"Experiment '{experiment_name}' does not exist. Attempting to create it.\")\n",
    "    experiment_id = mlflow.create_experiment(experiment_name)\n",
    "    mlflow.set_experiment(experiment_id)\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"example_param\", 42)\n",
    "    mlflow.log_metric(\"example_metric\", 0.99)\n",
    "    print(\"Run logged successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment2\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "# Function to convert categorical values like 'yes'/'no' and 'furnished' to integers\n",
    "def preprocess_categorical_columns(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':  # Check for categorical columns\n",
    "            # Convert 'yes'/'no' to 1/0\n",
    "            if df[col].isin(['yes', 'no']).any():\n",
    "                df[col] = df[col].map({'yes': 1, 'no': 0})\n",
    "            \n",
    "            # Convert 'furnished', 'semi-furnished', 'unfurnished' to 0, 1, 2\n",
    "            if df[col].isin(['unfurnished', 'semi-furnished', 'furnished']).any():\n",
    "                df[col] = df[col].map({'unfurnished': 0, 'semi-furnished': 1, 'furnished': 2})\n",
    "    return df\n",
    "\n",
    "# Apply conversion to training and test data\n",
    "X_train = preprocess_categorical_columns(X_train)\n",
    "X_test = preprocess_categorical_columns(X_test)\n",
    "\n",
    "# Define the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), X_train.select_dtypes(include=['float64', 'int64']).columns),  # Scale numerical features\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), X_train.select_dtypes(include=['object']).columns)  # One-hot encode categorical data\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Use XGBRegressor for regression tasks\n",
    "regressor = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Create the pipeline with preprocessing and regression model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', regressor)\n",
    "])\n",
    "\n",
    "# Start the MLFlow run\n",
    "name = \"experiment2_housing\"  # Set your run name\n",
    "with mlflow.start_run(run_name=name):\n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_test = pipeline.predict(X_test)\n",
    "    \n",
    "    # Log the model and metrics with MLFlow\n",
    "    mlflow.sklearn.log_model(pipeline, \"model\")\n",
    "    \n",
    "    # Log evaluation metrics\n",
    "    mse = mean_squared_error(y_test, y_pred_test)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "\n",
    "    # Log hyperparameters\n",
    "    mlflow.log_param(\"objective\", \"reg:squarederror\")\n",
    "\n",
    "    print(\"Model training and logging completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment3\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assume you have already loaded the data (X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Step 1: Feature Engineering\n",
    "\n",
    "# Example: Create new features by combining existing ones (e.g., ratios, differences, interaction terms)\n",
    "def feature_engineering(df):\n",
    "    # Combine existing features (e.g., creating a new ratio feature)\n",
    "    if 'feature1' in df.columns and 'feature2' in df.columns:\n",
    "        df['feature_ratio'] = df['feature1'] / (df['feature2'] + 1e-5)  # Avoid division by zero\n",
    "    \n",
    "    # Example of creating an interaction feature\n",
    "    if 'feature3' in df.columns and 'feature4' in df.columns:\n",
    "        df['interaction_feature'] = df['feature3'] * df['feature4']\n",
    "    \n",
    "    # Example: Create polynomial features (e.g., squared term)\n",
    "    if 'feature5' in df.columns:\n",
    "        df['feature5_squared'] = df['feature5'] ** 2\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering to training and test data\n",
    "X_train = feature_engineering(X_train)\n",
    "X_test = feature_engineering(X_test)\n",
    "\n",
    "# Step 2: Define the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), X_train.select_dtypes(include=['float64', 'int64']).columns),  # Scale numerical features\n",
    "        ('cat', OneHotEncoder(), X_train.select_dtypes(include=['object']).columns)  # Encode categorical features\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 3: Initialize the model (XGBRegressor for regression)\n",
    "model = XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Step 4: Create the pipeline with preprocessing and regression model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', model)\n",
    "])\n",
    "\n",
    "# Step 5: Start MLFlow run to log the model and results\n",
    "with mlflow.start_run(run_name=\"feature_engineering_experiment\"):\n",
    "    # Step 6: Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Step 7: Predictions\n",
    "    y_pred_test = pipeline.predict(X_test)\n",
    "    \n",
    "    # Step 8: Log model\n",
    "    mlflow.sklearn.log_model(pipeline, \"model\")\n",
    "    \n",
    "    # Step 9: Evaluate model performance (e.g., RMSE, MAE)\n",
    "    mse = mean_squared_error(y_test, y_pred_test)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Step 10: Log metrics in MLFlow\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"MSE\", mse)\n",
    "    \n",
    "    # Step 11: Log parameters if applicable (e.g., model parameters, feature engineering steps)\n",
    "    mlflow.log_param(\"model_type\", \"XGBRegressor\")\n",
    "    mlflow.log_param(\"feature_engineering_steps\", \"feature_ratio, interaction_feature, feature5_squared\")\n",
    "\n",
    "    # Log any other parameters relevant to the experiment\n",
    "    # Example: Logging hyperparameters (optional)\n",
    "    mlflow.log_param(\"learning_rate\", model.get_params().get('learning_rate'))\n",
    "    mlflow.log_param(\"max_depth\", model.get_params().get('max_depth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment4\n",
    "\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already loaded\n",
    "\n",
    "# Step 1: Perform Correlation Threshold feature selection\n",
    "\n",
    "def correlation_threshold(X, threshold=0.9):\n",
    "    # Compute the correlation matrix\n",
    "    corr_matrix = X.corr().abs()\n",
    "    \n",
    "    # Select upper triangle of correlation matrix to check for duplicate correlations\n",
    "    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    # Identify columns to drop\n",
    "    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]\n",
    "    \n",
    "    # Drop the correlated features\n",
    "    X_filtered_corr = X.drop(columns=to_drop)\n",
    "    \n",
    "    return X_filtered_corr, to_drop\n",
    "\n",
    "# Apply correlation threshold to X_train and X_test\n",
    "X_train_corr, dropped_corr = correlation_threshold(X_train, threshold=0.9)\n",
    "X_test_corr = X_test[X_train_corr.columns]  # Make sure test set has the same columns after dropping\n",
    "\n",
    "# Step 2: Perform Feature Importance-based selection using XGBRegressor\n",
    "\n",
    "# Train an XGBRegressor to get feature importances\n",
    "model = XGBRegressor(objective='reg:squarederror')\n",
    "model.fit(X_train_corr, y_train)\n",
    "\n",
    "# Get the feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Define the threshold for feature importance (e.g., keep features with importance > 0.01)\n",
    "important_features = X_train_corr.columns[feature_importances > 0.01]\n",
    "X_train_imp = X_train_corr[important_features]\n",
    "X_test_imp = X_test_corr[important_features]\n",
    "\n",
    "# Step 3: Perform Variance Threshold feature selection\n",
    "\n",
    "# Variance threshold to remove features with low variance\n",
    "variance_threshold = VarianceThreshold(threshold=0.01)  # 0.01 is a typical threshold, adjust as needed\n",
    "X_train_var = variance_threshold.fit_transform(X_train_imp)\n",
    "X_test_var = variance_threshold.transform(X_test_imp)\n",
    "\n",
    "# Convert the result back to DataFrame\n",
    "X_train_var_df = pd.DataFrame(X_train_var, columns=important_features[variance_threshold.get_support()])\n",
    "X_test_var_df = pd.DataFrame(X_test_var, columns=important_features[variance_threshold.get_support()])\n",
    "\n",
    "# Step 4: Create and train the model pipeline after feature selection\n",
    "\n",
    "# Define the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), X_train_var_df.select_dtypes(include=['float64', 'int64']).columns),  # Scale numerical features\n",
    "        ('cat', OneHotEncoder(), X_train_var_df.select_dtypes(include=['object']).columns)  # Encode categorical features\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the pipeline with preprocessing and regression model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', model)\n",
    "])\n",
    "\n",
    "# Step 5: Start MLFlow run to log the model and results\n",
    "with mlflow.start_run(run_name=\"feature_selection_experiment\"):\n",
    "    # Step 6: Train the model\n",
    "    pipeline.fit(X_train_var_df, y_train)\n",
    "    \n",
    "    # Step 7: Predictions\n",
    "    y_pred_test = pipeline.predict(X_test_var_df)\n",
    "    \n",
    "    # Step 8: Log model\n",
    "    mlflow.sklearn.log_model(pipeline, \"model\")\n",
    "    \n",
    "    # Step 9: Evaluate model performance (e.g., RMSE, MSE)\n",
    "    mse = mean_squared_error(y_test, y_pred_test)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Step 10: Log metrics in MLFlow\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"MSE\", mse)\n",
    "    \n",
    "    # Step 11: Log feature selection details\n",
    "    mlflow.log_param(\"feature_selection_method\", \"Correlation Threshold, Feature Importance, Variance Threshold\")\n",
    "    mlflow.log_param(\"correlation_threshold\", 0.9)\n",
    "    mlflow.log_param(\"feature_importance_threshold\", 0.01)\n",
    "    mlflow.log_param(\"variance_threshold\", 0.01)\n",
    "\n",
    "    # Log the dropped features for correlation and variance threshold\n",
    "    mlflow.log_param(\"dropped_features_corr\", str(dropped_corr))\n",
    "    mlflow.log_param(\"remaining_features_after_importance\", str(important_features.tolist()))\n",
    "    mlflow.log_param(\"remaining_features_after_variance\", str(X_train_var_df.columns.tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment5\n",
    "\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming you have already loaded the data (X_train, X_test, y_train, y_test)\n",
    "# Example data: X_train, y_train, X_test, y_test\n",
    "\n",
    "# Step 1: Preprocess the data (scaling features)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), X_train.select_dtypes(include=['float64', 'int64']).columns),  # Scale numerical features\n",
    "        ('cat', OneHotEncoder(), X_train.select_dtypes(include=['object']).columns)  # Encode categorical features\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 2: Apply PCA for dimensionality reduction\n",
    "pca = PCA()\n",
    "\n",
    "# Fit PCA on the training data (after preprocessing)\n",
    "X_train_scaled = preprocessor.fit_transform(X_train)\n",
    "pca.fit(X_train_scaled)\n",
    "\n",
    "# Step 3: Plot the scree plot to show the explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o', linestyle='--')\n",
    "plt.title(\"Scree Plot\")\n",
    "plt.xlabel(\"Principal Components\")\n",
    "plt.ylabel(\"Explained Variance Ratio\")\n",
    "plt.grid(True)\n",
    "plt.xticks(range(1, len(pca.explained_variance_ratio_) + 1))\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Determine the number of components to select based on the cumulative variance (e.g., 95% explained variance)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "num_components = np.argmax(cumulative_variance >= 0.95) + 1  # Choose the number of components that explain 95% variance\n",
    "\n",
    "print(f\"Number of components selected: {num_components}\")\n",
    "\n",
    "# Step 5: Create the pipeline with PCA and XGBRegressor\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('pca', PCA(n_components=num_components)),\n",
    "    ('regressor', XGBRegressor(objective='reg:squarederror'))\n",
    "])\n",
    "\n",
    "# Step 6: Start MLFlow run to log the model and results\n",
    "with mlflow.start_run(run_name=\"pca_experiment\"):\n",
    "    # Step 7: Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Step 8: Predictions\n",
    "    y_pred_test = pipeline.predict(X_test)\n",
    "    \n",
    "    # Step 9: Log the model\n",
    "    mlflow.sklearn.log_model(pipeline, \"model\")\n",
    "    \n",
    "    # Step 10: Evaluate model performance (e.g., RMSE, MSE)\n",
    "    mse = mean_squared_error(y_test, y_pred_test)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Step 11: Log metrics in MLFlow\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"MSE\", mse)\n",
    "    \n",
    "    # Step 12: Log PCA details\n",
    "    mlflow.log_param(\"num_components_selected\", num_components)\n",
    "    mlflow.log_param(\"explained_variance_threshold\", 0.95)\n",
    "\n",
    "    # Log the cumulative explained variance for analysis\n",
    "    mlflow.log_metric(\"cumulative_explained_variance\", cumulative_variance[-1])\n",
    "\n",
    "    # Save the scree plot as an image and log it in MLFlow\n",
    "    scree_plot_path = \"scree_plot.png\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o', linestyle='--')\n",
    "    plt.title(\"Scree Plot\")\n",
    "    plt.xlabel(\"Principal Components\")\n",
    "    plt.ylabel(\"Explained Variance Ratio\")\n",
    "    plt.grid(True)\n",
    "    plt.xticks(range(1, len(pca.explained_variance_ratio_) + 1))\n",
    "    plt.savefig(scree_plot_path)\n",
    "    mlflow.log_artifact(scree_plot_path)\n",
    "\n",
    "    print(f\"Model trained with {num_components} components and logged to MLFlow.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment6\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming you have already loaded the data (X_train, X_test, y_train, y_test)\n",
    "# Example data: X_train, y_train, X_test, y_test\n",
    "\n",
    "# Step 1: Feature Engineering - Create new interaction and polynomial features\n",
    "def feature_engineering(df):\n",
    "    # Ensure feature3 and feature4 exist before applying PolynomialFeatures\n",
    "    if 'feature3' in df.columns and 'feature4' in df.columns:\n",
    "        # Create polynomial features (quadratic features for example)\n",
    "        poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "        poly_features = poly.fit_transform(df[['feature3', 'feature4']])\n",
    "        poly_df = pd.DataFrame(poly_features, columns=poly.get_feature_names_out(['feature3', 'feature4']))\n",
    "        df = pd.concat([df, poly_df], axis=1)\n",
    "    else:\n",
    "        print(\"Warning: 'feature3' and/or 'feature4' are missing from the dataset.\")\n",
    "\n",
    "    # Create interaction terms between two features (make sure they exist)\n",
    "    if 'feature1' in df.columns and 'feature2' in df.columns:\n",
    "        df['feature_interaction'] = df['feature1'] * df['feature2']\n",
    "    else:\n",
    "        print(\"Warning: 'feature1' and/or 'feature2' are missing from the dataset.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering to training and test data\n",
    "X_train = feature_engineering(X_train)\n",
    "X_test = feature_engineering(X_test)\n",
    "\n",
    "# Step 2: Preprocessing pipeline for numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), X_train.select_dtypes(include=['float64', 'int64']).columns),  # Scale numerical features\n",
    "        ('cat', OneHotEncoder(), X_train.select_dtypes(include=['object']).columns)  # Encode categorical features\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 3: Define models to compare: XGBRegressor, RandomForestRegressor, and LinearRegression\n",
    "models = {\n",
    "    'XGBRegressor': XGBRegressor(objective='reg:squarederror'),\n",
    "    'RandomForestRegressor': RandomForestRegressor(),\n",
    "    'LinearRegression': LinearRegression()\n",
    "}\n",
    "\n",
    "# Step 4: Define hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'XGBRegressor': {\n",
    "        'regressor__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'regressor__max_depth': [3, 6, 9],\n",
    "        'regressor__n_estimators': [100, 200]\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'regressor__n_estimators': [100, 200],\n",
    "        'regressor__max_depth': [10, 20, None],\n",
    "        'regressor__min_samples_split': [2, 5]\n",
    "    },\n",
    "    'LinearRegression': {\n",
    "        # Linear Regression doesn't have hyperparameters to tune, but can still be included for comparison\n",
    "        'regressor__fit_intercept': [True, False]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Step 5: Train models using GridSearchCV to optimize hyperparameters\n",
    "best_models = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    # Create the pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', model)\n",
    "    ])\n",
    "    \n",
    "    # Perform GridSearchCV with n_jobs=1 to avoid parallelism and serialization issues\n",
    "    grid_search = GridSearchCV(pipeline, param_grid[model_name], cv=5, n_jobs=1, verbose=1, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Store the best model for later comparison\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "    \n",
    "    # Log the model and hyperparameters in MLFlow\n",
    "    with mlflow.start_run(run_name=f'{model_name}_experiment'):\n",
    "        mlflow.sklearn.log_model(grid_search.best_estimator_, f'{model_name}_model')\n",
    "        mlflow.log_params(grid_search.best_params_)\n",
    "        \n",
    "        # Step 6: Predictions and Evaluation\n",
    "        y_pred_test = grid_search.best_estimator_.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred_test)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        mlflow.log_metric(\"RMSE\", rmse)\n",
    "        mlflow.log_metric(\"MSE\", mse)\n",
    "        \n",
    "        print(f\"{model_name} RMSE: {rmse}\")\n",
    "        print(f\"{model_name} MSE: {mse}\")\n",
    "\n",
    "# Step 7: Compare the models' performance (display results)\n",
    "for model_name, best_model in best_models.items():\n",
    "    print(f\"Best {model_name} model: {best_model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment7\n",
    "\n",
    "\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already loaded\n",
    "# Example data: X_train, y_train, X_test, y_test\n",
    "\n",
    "# Step 1: Feature Engineering - Optional (You can add feature engineering here)\n",
    "def feature_engineering(df):\n",
    "    # Example feature engineering: you can modify or create new features here if needed\n",
    "    return df\n",
    "\n",
    "# Apply feature engineering if necessary\n",
    "X_train = feature_engineering(X_train)\n",
    "X_test = feature_engineering(X_test)\n",
    "\n",
    "# Step 2: Preprocessing pipeline for numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), X_train.select_dtypes(include=['float64', 'int64']).columns),  # Scale numerical features\n",
    "        ('cat', OneHotEncoder(), X_train.select_dtypes(include=['object']).columns)  # Encode categorical features\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 3: Define models to compare: XGBRegressor, RandomForestRegressor, and LinearRegression\n",
    "models = {\n",
    "    'XGBRegressor': XGBRegressor(objective='reg:squarederror'),\n",
    "    'RandomForestRegressor': RandomForestRegressor(),\n",
    "    'LinearRegression': LinearRegression()\n",
    "}\n",
    "\n",
    "# Step 4: Define the number of features to select in RFE (e.g., select 10 features)\n",
    "n_features_to_select = 10\n",
    "\n",
    "# Step 5: Train models using RFE for feature selection and evaluate performance\n",
    "best_models = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    # Create the pipeline with RFE for feature selection\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('feature_selection', RFE(estimator=model, n_features_to_select=n_features_to_select, step=1)),\n",
    "        ('regressor', model)\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Step 6: Predictions and Evaluation\n",
    "    y_pred_test = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate RMSE, MAE, and R²\n",
    "    mse = mean_squared_error(y_test, y_pred_test)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    # Log the model and hyperparameters in MLFlow\n",
    "    with mlflow.start_run(run_name=f'{model_name}_experiment_with_RFE'):\n",
    "        mlflow.sklearn.log_model(pipeline, f'{model_name}_model')\n",
    "        mlflow.log_param(\"n_features_to_select\", n_features_to_select)\n",
    "        mlflow.log_params({'model_type': model_name})\n",
    "        \n",
    "        mlflow.log_metric(\"RMSE\", rmse)\n",
    "        mlflow.log_metric(\"MAE\", mae)\n",
    "        mlflow.log_metric(\"R2\", r2)\n",
    "        \n",
    "        print(f\"{model_name} RMSE: {rmse}\")\n",
    "        print(f\"{model_name} MAE: {mae}\")\n",
    "        print(f\"{model_name} R²: {r2}\")\n",
    "\n",
    "    # Store the best model for later comparison\n",
    "    best_models[model_name] = pipeline\n",
    "\n",
    "# Step 7: Compare the models' performance (display results)\n",
    "for model_name, best_model in best_models.items():\n",
    "    print(f\"Best {model_name} model: {best_model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F1ScorePlots\n",
    "\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# Assuming you have already loaded the data (X_train, X_test, y_train, y_test)\n",
    "# Example data: X_train, y_train, X_test, y_test\n",
    "\n",
    "# Step 1: Feature Engineering - Optional (You can add feature engineering here if needed)\n",
    "def feature_engineering(df):\n",
    "    # Example feature engineering: you can modify or create new features here if needed\n",
    "    return df\n",
    "\n",
    "# Apply feature engineering if necessary\n",
    "X_train = feature_engineering(X_train)\n",
    "X_test = feature_engineering(X_test)\n",
    "\n",
    "# Step 2: Preprocessing pipeline for numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), X_train.select_dtypes(include=['float64', 'int64']).columns),  # Scale numerical features\n",
    "        ('cat', OneHotEncoder(), X_train.select_dtypes(include=['object']).columns)  # Encode categorical features\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 3: Define models to compare: XGBRegressor, RandomForestRegressor, and LinearRegression\n",
    "models = {\n",
    "    'XGBRegressor': XGBRegressor(objective='reg:squarederror'),\n",
    "    'RandomForestRegressor': RandomForestRegressor(),\n",
    "    'LinearRegression': LinearRegression()\n",
    "}\n",
    "\n",
    "# Step 4: Define the number of features to select in RFE (e.g., select 10 features)\n",
    "n_features_to_select = 10\n",
    "\n",
    "# Step 5: Train models using RFE for feature selection and evaluate F1-score\n",
    "best_models = {}\n",
    "f1_scores = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    # Create the pipeline with RFE for feature selection\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('feature_selection', RFE(estimator=model, n_features_to_select=n_features_to_select, step=1)),\n",
    "        ('regressor', model)\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Step 6: Predictions and Evaluation (Convert y_test into categories for classification task)\n",
    "    y_pred_test = pipeline.predict(X_test)\n",
    "\n",
    "    # Convert continuous values into discrete classes (for demonstration purposes)\n",
    "    discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n",
    "    y_test_binned = discretizer.fit_transform(y_test.values.reshape(-1, 1)).flatten()\n",
    "    y_pred_test_binned = discretizer.transform(y_pred_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Compute F1-score\n",
    "    f1 = f1_score(y_test_binned, y_pred_test_binned, average='weighted')\n",
    "    \n",
    "    # Log the model and hyperparameters in MLFlow\n",
    "    with mlflow.start_run(run_name=f'{model_name}_experiment_with_RFE'):\n",
    "        mlflow.sklearn.log_model(pipeline, f'{model_name}_model')\n",
    "        mlflow.log_param(\"n_features_to_select\", n_features_to_select)\n",
    "        mlflow.log_params({'model_type': model_name})\n",
    "        \n",
    "        mlflow.log_metric(\"F1-Score\", f1)\n",
    "        \n",
    "        print(f\"{model_name} F1-Score: {f1}\")\n",
    "        \n",
    "    # Store the best model for later comparison\n",
    "    best_models[model_name] = pipeline\n",
    "    f1_scores[model_name] = f1\n",
    "\n",
    "# Step 7: Compare the models' performance (F1-scores plot)\n",
    "model_names = list(f1_scores.keys())\n",
    "f1_values = list(f1_scores.values())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(model_names, f1_values, color='skyblue')\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"F1-Score\")\n",
    "plt.title(\"F1-Score Comparison of Models\")\n",
    "plt.show()\n",
    "\n",
    "# Step 8: Log the best model based on F1-Score\n",
    "best_model_name = max(f1_scores, key=f1_scores.get)\n",
    "print(f\"The best model based on F1-Score is: {best_model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the best model using joblib\n",
    "best_model_name = max(f1_scores, key=f1_scores.get)  # Get the model with the best F1-score\n",
    "best_model = best_models[best_model_name]\n",
    "\n",
    "# Save the model to a file\n",
    "joblib.dump(best_model, 'best_model.joblib')\n",
    "print(f\"Model saved as 'best_model.joblib'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Define your input model\n",
    "class YourInputModel(BaseModel):\n",
    "    size: float\n",
    "    rooms: int\n",
    "    location: str\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Root endpoint\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\"message\": \"Welcome to the FastAPI application\"}\n",
    "\n",
    "# Predict endpoint\n",
    "@app.post(\"/predict\")\n",
    "def predict(data: YourInputModel):\n",
    "    # Example prediction logic\n",
    "    prediction = (data.size * 1000) + (data.rooms * 500)  # Replace with your actual model logic\n",
    "    return {\"prediction\": prediction}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot more that you can do with outputs (such as including interactive outputs)\n",
    "with your book. For more information about this, see [the Jupyter Book documentation](https://jupyterbook.org)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
